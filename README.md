# Transformer Develop

Purpose is explain encoder + decoder structure and attention architecture with details plus word data embedding step by step.

TODO

* Attention
  * Self-attention
  * Multi-head attention
* Positional encoding

* Encoder
* Decoder
# transformer_develop
